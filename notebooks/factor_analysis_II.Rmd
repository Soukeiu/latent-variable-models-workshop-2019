---
title: "Factor Analysis"
output: 
  html_notebook: 
    code_folding: none
    highlight: pygments
    theme: sandstone
editor_options: 
  chunk_output_type: inline
---


## Initialization

This assumes the prior Rmd files have been run.  See the README file.

```{r misc_functions}
source('functions_scripts/functions.R')
```

```{r load_pacages, message=FALSE}
library(tidyverse)
library(psych)
library(lavaan)
library(broom)
```


## Structural Equation Modeling

Structural Equation Modeling (SEM) takes a more theoretically based approach to modeling observed and latent variables than we have used thus far, and includes indirect effects, multiple outcomes, and more.  Most any model you have conducted can be seen as a special case of SEM, but SEM tools are generally geared towards specific kinds of models.

## Latent Variable Modeling

We can take a first step into the SEM by sticking with standard factor analysis.  In our previous example we did a factor analysis for two factors of the BFI.  We will do so again here, but instead of letting every observed variable load on every factor, we will use our substantive knowledge that tells us some variables should be more correlated with one another than others. Some call this 'confirmatory factor analysis' as opposed to 'exploratory factor analysis', but this is not a very useful distinction in my opinion.  The CFA is just a reduced or constrained model relative to the EFA.  The EFA is more realistic (no loading is truly 0), but the CFA is cleaner and more easily interpreted.

To run the following model we'll use `lavaan` and its `cfa` function.  We'll first reduce the `bfi` data to two item sets of agreeableness and neuroticism.  We'll also reverse score the items to get consistent/positive loadings.  The notebook may have problems displaying everything inline, so you may need to look to your console or preview to see all the output.

```{r bfi_agree_neuro}
bfi_agree_neuro = bfi %>% 
  select(matches('E[1-5]|N[1-5]')) %>% 
  mutate(E2 = -E2, E1 = -E1)

modelCode = "
  agree  =~ E1 + E2 + E3 + E4 + E5
  neurot =~ N1 + N2 + N3 + N4 + N5

  # specify correlated factors; not necessary
  # agree ~~ neurot

  # if you don't want them correlated
  # agree ~~ 0*neurot
"

cfa_bfi = cfa(modelCode, data=bfi_agree_neuro)
summary(cfa_bfi, standardized=T, rsq=T, fit=T, nd=2)
```



### Common Measures of Fit

There are many, many measures of model fit  when one moves to the SEM realm, and none of them will give you a definitive answer as to how your model is doing.  Your assessment, should be based on a holistic approach to get a global sense of how your model is doing.  Let's loo again at the alienation results.

What follows is a brief summary.


```{r cfa_fit}
glance(cfa_bfi) %>% mutate_if(is.numeric, round, digits = 2)
```


##### Chi-square test


Conceptually, the $\chi^2$ test measures the discrepancy between the observed correlations and those implied by the model.  This test compares the fitted model with a (saturated) model that does not have any degrees of freedom.   A non-significant $\chi^2$ suggests our model-implied correlations are not statistically different from those we observe. However, many things that affect this measure (and those using it), including:

- multivariate non-normality: can 'help' or hurt depending on the nature of it
- the size of the correlations: larger ones are typically related to larger predicted vs. observed discrepancies
- unreliable measures: can actually make this test fail to reject
- sample size: same as with any other model scenario and statistical significance

##### CFI etc.

The **Comparative Fit Index** compares the fitted model to a null model that assumes there is no relationship among the measured items (i.e. complete independence). It ranges from 0 to 1, or rather it is rounded to fall between 0 and 1, and CFI values larger than .9 or especially .95 are typically desired. Another one very commonly provided includes the **Tucker-Lewis Fit Index**, which is provided in standard **lavaan** output, but there are more **incremental fit indices** where those come from.

##### RMSEA

The **root mean squared error of approximation** is a measure that also centers on the model-implied vs. sample covariance matrix, and, all else being equal, is lower for simpler models and larger sample sizes. Maybe loo for values less than .05, but again, don't be rigid with this.  **Lavaan** also provides a one-sided test that the RMSEA is $\leq .05$, or 'test of close fit', for which the corresponding p-value ideally would be high.  The confidence interval is enough for reporting purposes, and an upper bound beyond .10 may indicate a problem.

##### SRMR

The **standardized root mean squared residual** is the mean absolute correlation residual, i.e. the difference between the observed and model-implied correlations. Historical suggestions are to also loo for values less than .05, but it is better to simply inspect the residuals and note where there are large discrepancies.


##### Fit Summarized

A brief summary of these and other old/typical measures of fit are described [here](https://en.wiipedia.org/wii/Confirmatory_factor_analysis#Evaluating_model_fit).  However they all have issues, and one should *never* use cutoffs as a basis for your ultimate thinking about model performance.  Studies have been done and all the fit indices can potentially have problems in various scenarios, and the cutoffs commonly used by applied researchers do not hold up under scrutiny.  While they can provide some assistance in the process, they are not meant to replace a global assessment of theory-result compatibility.  


#### Model Comparison

AIC and BIC can be used for model comparison.  In clear cases of hierarchical models, one can use likelihood ratio tests as well.


```{r cfa_aic}
AIC(cfa_bfi)
```


### Loadings

Generally we report the standardized loadings, where both the latent variables and observed variables are standardized.  This is the `std.all` column, and these loadings are interpreted as before.  For example, we see the negatively valued loading for the reverse-scored item, as well as it being the lowest loading item (not an uncommon occurrence).  Other loadings seem more or less satisfactory by typical standards, but might be somewhat low for scale development.  In general, neuroticism seems a better scale than agreeableness.

```{r cfa_loadings}
tidy(cfa_bfi) %>% 
  select(term, std.all) %>% 
  filter(grepl(term, pattern = 'agree =~ |neurot =~')) %>% 
  mutate(std.all = round(std.all, 2))
```


### Other

Aside from loadings and fit, we get the estimated correlations of the factors (`agree ~~ neurot`), the (residual) variances of the items (the uniqueness from our previous approach) and latent variables (`Variances`), and the average R-squared of each item explained by the factors (`R-Square`).

## Measurement Models First!

If you do end up conducting SEM with latent variables for more complicated models, you still need to run the basic factor analyses, usually separately for each latent variable under consideration.  If things are not holding up well there, any analysis using them will not do well either.

## No Double-Dipping

Doing a reduced factor analysis after an exploratory one, and then only presenting the trimmed one as your main result is misleading and basically a sure way to overfit your data.  It would be okay if you do the reduced one on new data, but this unfortunately is very rarely done.